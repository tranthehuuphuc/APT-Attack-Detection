{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è APT Attack Detection - Complete System Management\n",
    "\n",
    "**Comprehensive notebook for managing the entire APT Detection system**\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. Environment Setup & Verification\n",
    "2. Data Preparation (MITRE, CTI, Events)\n",
    "3. Engine Bootstrap & Dataset Linking\n",
    "4. CTI Agent Pipeline\n",
    "5. Hunting Pipeline (Demo & Real-time)\n",
    "6. Training Pipeline\n",
    "7. Evaluation & Metrics\n",
    "8. Visualization & Analysis\n",
    "9. Troubleshooting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup & Verification\n",
    "\n",
    "### Check Python Version & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_python",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"\\nCurrent directory: {Path.cwd()}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if not (Path.cwd() / 'src').exists():\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Not in APT-Attack-Detection directory!\")\n",
    "    print(\"Please navigate to the repo root first.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ In correct directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_core",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q -r requirements/core.txt\n",
    "print(\"‚úÖ Core dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install agent dependencies\n",
    "!pip install -q -r requirements/agent.txt\n",
    "print(\"‚úÖ Agent dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_g4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install g4f (optional - for free LLM backend)\n",
    "try:\n",
    "    !pip install -q -r requirements/g4f.txt\n",
    "    print(\"‚úÖ g4f installed (free LLM backend available)\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  g4f installation failed (optional - you can use OpenAI instead)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key imports\n",
    "import networkx as nx\n",
    "import yaml\n",
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    print(\"‚úÖ OpenAI library available\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  OpenAI not available (install: pip install openai)\")\n",
    "\n",
    "try:\n",
    "    import g4f\n",
    "    print(\"‚úÖ g4f available (free backend)\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è  g4f not available (optional)\")\n",
    "\n",
    "print(\"\\n‚úÖ All essential imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_2",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Preparation\n",
    "\n",
    "### 2.1 Download MITRE ATT&CK STIX Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_mitre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "mitre_dir = Path(\"data/mitre\")\n",
    "mitre_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stix_file = mitre_dir / \"enterprise-attack.json\"\n",
    "\n",
    "if stix_file.exists():\n",
    "    print(f\"‚úÖ MITRE ATT&CK STIX already exists: {stix_file}\")\n",
    "    print(f\"   Size: {stix_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(\"Downloading MITRE ATT&CK STIX...\")\n",
    "    url = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n",
    "    \n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    stix_file.write_bytes(response.content)\n",
    "    print(f\"‚úÖ Downloaded: {stix_file}\")\n",
    "    print(f\"   Size: {len(response.content) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Verify the file\n",
    "import json\n",
    "stix_data = json.loads(stix_file.read_text())\n",
    "techniques = [obj for obj in stix_data['objects'] if obj.get('type') == 'attack-pattern']\n",
    "print(f\"\\n‚úÖ Loaded {len(techniques)} ATT&CK techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_2_2",
   "metadata": {},
   "source": [
    "### 2.2 Setup CTI RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_rss",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cti_dir = Path(\"data/cti_reports\")\n",
    "cti_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rss_file = cti_dir / \"rss_seeds.txt\"\n",
    "\n",
    "# Default RSS feeds for cyber threat intelligence\n",
    "default_feeds = [\n",
    "    \"# Government & Official Sources\",\n",
    "    \"https://www.cisa.gov/cybersecurity-advisories/all.xml\",\n",
    "    \"https://www.us-cert.gov/ncas/current-activity.xml\",\n",
    "    \"\",\n",
    "    \"# Security News\",\n",
    "    \"https://www.bleepingcomputer.com/feed/\",\n",
    "    \"https://thehackernews.com/feeds/posts/default\",\n",
    "    \"https://feeds.feedburner.com/TheHackersNews\",\n",
    "    \"\",\n",
    "    \"# Threat Research\",\n",
    "    \"https://www.crowdstrike.com/blog/feed/\",\n",
    "    \"https://www.fireeye.com/blog/threat-research.html/feed\",\n",
    "    \"# Add more feeds as needed\",\n",
    "]\n",
    "\n",
    "if rss_file.exists():\n",
    "    print(f\"‚úÖ RSS feeds file already exists: {rss_file}\")\n",
    "    feeds = [line.strip() for line in rss_file.read_text().splitlines() \n",
    "             if line.strip() and not line.strip().startswith('#')]\n",
    "    print(f\"   Contains {len(feeds)} active feeds\")\n",
    "else:\n",
    "    rss_file.write_text(\"\\n\".join(default_feeds))\n",
    "    print(f\"‚úÖ Created RSS feeds file: {rss_file}\")\n",
    "    print(f\"   Added {len([f for f in default_feeds if f and not f.startswith('#')])} default feeds\")\n",
    "\n",
    "print(\"\\nüì° RSS Feeds:\")\n",
    "print(rss_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_2_3",
   "metadata": {},
   "source": [
    "### 2.3 Create Sample Events (for Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_sample_events",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "events_dir = Path(\"runs/events\")\n",
    "events_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "events_file = events_dir / \"events.jsonl\"\n",
    "\n",
    "# Create sample suspicious events\n",
    "sample_events = [\n",
    "    # Suspicious process in /tmp\n",
    "    {\n",
    "        \"kind\": \"process_start\",\n",
    "        \"ts\": time.time(),\n",
    "        \"pid\": 1234,\n",
    "        \"ppid\": 1000,\n",
    "        \"exe\": \"/tmp/malicious_binary\",\n",
    "        \"comm\": \"malicious_binary\"\n",
    "    },\n",
    "    # File write to suspicious location\n",
    "    {\n",
    "        \"kind\": \"file_op\",\n",
    "        \"ts\": time.time() + 1,\n",
    "        \"pid\": 1234,\n",
    "        \"exe\": \"/tmp/malicious_binary\",\n",
    "        \"comm\": \"malicious_binary\",\n",
    "        \"path\": \"/tmp/.hidden_payload\",\n",
    "        \"action\": \"WRITE\"\n",
    "    },\n",
    "    # Network connection\n",
    "    {\n",
    "        \"kind\": \"net_op\",\n",
    "        \"ts\": time.time() + 2,\n",
    "        \"pid\": 1234,\n",
    "        \"exe\": \"/tmp/malicious_binary\",\n",
    "        \"comm\": \"malicious_binary\",\n",
    "        \"saddr\": \"192.168.1.100:8080\"\n",
    "    },\n",
    "    # Normal process for comparison\n",
    "    {\n",
    "        \"kind\": \"process_start\",\n",
    "        \"ts\": time.time() + 3,\n",
    "        \"pid\": 5678,\n",
    "        \"ppid\": 1,\n",
    "        \"exe\": \"/usr/bin/bash\",\n",
    "        \"comm\": \"bash\"\n",
    "    },\n",
    "]\n",
    "\n",
    "with events_file.open('w') as f:\n",
    "    for event in sample_events:\n",
    "        f.write(json.dumps(event) + '\\n')\n",
    "\n",
    "print(f\"‚úÖ Created sample events: {events_file}\")\n",
    "print(f\"   Events count: {len(sample_events)}\")\n",
    "print(\"\\nüìã Sample events preview:\")\n",
    "for i, ev in enumerate(sample_events[:3], 1):\n",
    "    print(f\"{i}. {ev['kind']}: {ev.get('exe', ev.get('path', 'N/A'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_3",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Engine Bootstrap & Dataset Linking\n",
    "\n",
    "### 3.1 Bootstrap GNN Engine (Manual Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "engine_dir = Path(\"src/engine/graph_matcher/engine_repo\")\n",
    "\n",
    "if (engine_dir / \"src\").exists():\n",
    "    print(f\"‚úÖ Engine repository exists at: {engine_dir}\")\n",
    "    print(f\"   Files: {list(engine_dir.glob('*'))}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Engine repository NOT found\")\n",
    "    print(\"\\nüìù To bootstrap the engine, run:\")\n",
    "    print(\"\\n  bash scripts/bootstrap_engine.sh <MEGR_APT_GIT_URL>\")\n",
    "    print(\"\\n  Replace <MEGR_APT_GIT_URL> with the actual repository URL\")\n",
    "    print(\"\\n‚ö†Ô∏è  Without the engine, you CAN'T run training or full hunting pipeline\")\n",
    "    print(\"   But CTI Agent pipeline will work fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_3_2",
   "metadata": {},
   "source": [
    "### 3.2 Link DARPA TC Datasets (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "link_datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "engine_dataset_dir = Path(\"src/engine/graph_matcher/engine_repo/dataset\")\n",
    "target_dir = Path(\"data/datasets\")\n",
    "\n",
    "if engine_dataset_dir.exists():\n",
    "    print(\"Linking DARPA TC datasets...\")\n",
    "    result = subprocess.run([\"bash\", \"scripts/link_tc_datasets.sh\"], \n",
    "                          capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Dataset linking completed\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Linking failed: {result.stderr}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Engine datasets not available\")\n",
    "    print(\"   This is OK for CTI Agent testing\")\n",
    "    print(\"   Required only for training and full hunting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ CTI Agent Pipeline\n",
    "\n",
    "### 4.1 Configure LLM Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use OpenAI (requires API key)\n",
    "# Uncomment and set your API key:\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "# os.environ['OPENAI_MODEL'] = 'gpt-4o-mini'  # or gpt-4, gpt-3.5-turbo\n",
    "# llm_backend = 'openai'\n",
    "\n",
    "# Option 2: Use g4f (free, no API key)\n",
    "llm_backend = 'g4f'\n",
    "\n",
    "print(f\"ü§ñ LLM Backend: {llm_backend}\")\n",
    "\n",
    "if llm_backend == 'openai':\n",
    "    if os.getenv('OPENAI_API_KEY'):\n",
    "        print(\"‚úÖ OpenAI API key configured\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  OPENAI_API_KEY not set!\")\n",
    "        print(\"   Set it above or use g4f instead\")\n",
    "elif llm_backend == 'g4f':\n",
    "    try:\n",
    "        import g4f\n",
    "        print(\"‚úÖ g4f available\")\n",
    "    except:\n",
    "        print(\"‚ùå g4f not installed\")\n",
    "        print(\"   Install: pip install -r requirements/g4f.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_4_2",
   "metadata": {},
   "source": [
    "### 4.2 Run CTI Agent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"-m\", \"src.pipeline.agent.main\",\n",
    "    \"--rss-file\", \"data/cti_reports/rss_seeds.txt\",\n",
    "    \"--stix\", \"data/mitre/enterprise-attack.json\",\n",
    "    \"--out-cti\", \"runs/cti\",\n",
    "    \"--out-qg\", \"data/query_graphs\",\n",
    "    \"--out-seeds\", \"runs/cti/seeds.json\",\n",
    "    \"--llm-backend\", llm_backend,\n",
    "    \"--per-source-limit\", \"3\",  # Limit to 3 items per feed for demo\n",
    "]\n",
    "\n",
    "print(f\"üöÄ Running CTI Agent with {llm_backend} backend...\")\n",
    "print(f\"Command: {' '.join(cmd)}\\n\")\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"\\nSTDERR:\")\n",
    "    print(result.stderr)\n",
    "else:\n",
    "    print(\"\\n‚úÖ CTI Agent completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_4_3",
   "metadata": {},
   "source": [
    "### 4.3 Inspect CTI Agent Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_cti_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Check seeds.json\n",
    "seeds_file = Path(\"runs/cti/seeds.json\")\n",
    "if seeds_file.exists():\n",
    "    seeds = json.loads(seeds_file.read_text())\n",
    "    print(f\"üéØ CTI Seeds Summary:\")\n",
    "    print(f\"   Techniques: {len(seeds.get('techniques', []))}\")\n",
    "    print(f\"   Indicators: {len(seeds.get('indicators', []))}\")\n",
    "    \n",
    "    print(\"\\nüìã Top 5 Techniques:\")\n",
    "    for i, tech in enumerate(seeds.get('techniques', [])[:5], 1):\n",
    "        tid = tech.get('technique_id', 'N/A')\n",
    "        conf = tech.get('confidence', 0)\n",
    "        print(f\"   {i}. {tid} (confidence: {conf:.2f})\")\n",
    "    \n",
    "    print(\"\\nüîç Top 5 Indicators:\")\n",
    "    for i, ind in enumerate(seeds.get('indicators', [])[:5], 1):\n",
    "        itype = ind.get('type', 'N/A')\n",
    "        value = ind.get('value', 'N/A')[:50]\n",
    "        print(f\"   {i}. [{itype}] {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  seeds.json not found\")\n",
    "\n",
    "# 2. Check query graphs\n",
    "qg_dir = Path(\"data/query_graphs\")\n",
    "if qg_dir.exists():\n",
    "    qg_files = list(qg_dir.glob(\"*.json\"))\n",
    "    print(f\"\\nüìä Query Graphs: {len(qg_files)} generated\")\n",
    "    for qg in qg_files[:5]:\n",
    "        print(f\"   - {qg.name}\")\n",
    "\n",
    "# 3. Check CTI items\n",
    "cti_dir = Path(\"runs/cti\")\n",
    "if cti_dir.exists():\n",
    "    cti_files = list(cti_dir.glob(\"cti_*.json\"))\n",
    "    print(f\"\\nüì∞ CTI Items: {len(cti_files)} processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_5",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Hunting Pipeline\n",
    "\n",
    "### 5.1 Visualize Provenance Graph (from Sample Events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_provenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "from src.common.io import read_jsonl\n",
    "from src.pipeline.hunting.provenance import WindowedProvenanceGraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build provenance graph from sample events\n",
    "pg = WindowedProvenanceGraph(window_seconds=300, max_nodes=10000)\n",
    "\n",
    "events_file = Path(\"runs/events/events.jsonl\")\n",
    "if events_file.exists():\n",
    "    for ev in read_jsonl(events_file):\n",
    "        pg.ingest(ev)\n",
    "    \n",
    "    print(f\"üåê Provenance Graph:\")\n",
    "    print(f\"   Nodes: {pg.g.number_of_nodes()}\")\n",
    "    print(f\"   Edges: {pg.g.number_of_edges()}\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(pg.g, k=2, iterations=50)\n",
    "    \n",
    "    # Color nodes by type\n",
    "    colors = []\n",
    "    for node in pg.g.nodes():\n",
    "        ntype = pg.g.nodes[node].get('ntype', 'unknown')\n",
    "        if ntype == 'process':\n",
    "            colors.append('lightblue')\n",
    "        elif ntype == 'file':\n",
    "            colors.append('lightgreen')\n",
    "        elif ntype == 'socket':\n",
    "            colors.append('orange')\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "    \n",
    "    nx.draw(pg.g, pos, node_color=colors, with_labels=True, \n",
    "            node_size=1000, font_size=8, arrows=True)\n",
    "    \n",
    "    plt.title(\"Provenance Graph Visualization\")\n",
    "    plt.legend(['Process (blue)', 'File (green)', 'Socket (orange)'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print node details\n",
    "    print(\"\\nüìã Nodes Detail:\")\n",
    "    for node, data in pg.g.nodes(data=True):\n",
    "        ntype = data.get('ntype', 'unknown')\n",
    "        if ntype == 'process':\n",
    "            print(f\"   {node} [{ntype}]: {data.get('exe', 'N/A')}\")\n",
    "        elif ntype == 'file':\n",
    "            print(f\"   {node} [{ntype}]: {data.get('path', 'N/A')}\")\n",
    "        elif ntype == 'socket':\n",
    "            print(f\"   {node} [{ntype}]: {data.get('saddr', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No events file found. Create sample events first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_5_2",
   "metadata": {},
   "source": [
    "### 5.2 Find Seeds (CTI-based + Heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find_seeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.hunting.seeding import find_seeds\n",
    "\n",
    "if pg.g.number_of_nodes() > 0:\n",
    "    seeds = find_seeds(\n",
    "        pg.g, \n",
    "        query_name=\"demo\",\n",
    "        cti_seeds_path=\"runs/cti/seeds.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Seed Nodes Found: {len(seeds)}\")\n",
    "    for i, seed in enumerate(seeds, 1):\n",
    "        node_data = pg.g.nodes[seed]\n",
    "        ntype = node_data.get('ntype', 'unknown')\n",
    "        if ntype == 'process':\n",
    "            info = node_data.get('exe', 'N/A')\n",
    "        elif ntype == 'file':\n",
    "            info = node_data.get('path', 'N/A')\n",
    "        else:\n",
    "            info = node_data.get('saddr', 'N/A')\n",
    "        print(f\"   {i}. {seed} [{ntype}]: {info}\")\n",
    "    \n",
    "    if seeds:\n",
    "        print(\"\\n‚úÖ Seeds identified - ready for subgraph extraction\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No seeds found (this is OK for benign events)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No graph to search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_5_3",
   "metadata": {},
   "source": [
    "### 5.3 Run Full Hunting Pipeline (Requires Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_hunting",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_exists = (Path(\"src/engine/graph_matcher/engine_repo/src\").exists())\n",
    "checkpoint_exists = len(list(Path(\"runs/checkpoints\").glob(\"*.pt\"))) > 0 if Path(\"runs/checkpoints\").exists() else False\n",
    "\n",
    "if engine_exists and checkpoint_exists:\n",
    "    print(\"üöÄ Running hunting pipeline...\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"src.pipeline.hunting.main\",\n",
    "        \"--dataset\", \"cadets\",\n",
    "        \"--events\", \"runs/events/events.jsonl\",\n",
    "        \"--checkpoint\", str(list(Path(\"runs/checkpoints\").glob(\"*.pt\"))[0]),\n",
    "        \"--query-name\", \"demo\",\n",
    "        \"--cti-seeds\", \"runs/cti/seeds.json\",\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        print(\"\\nErrors:\")\n",
    "        print(result.stderr)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Cannot run full hunting pipeline:\")\n",
    "    if not engine_exists:\n",
    "        print(\"   - Engine not installed (see section 3.1)\")\n",
    "    if not checkpoint_exists:\n",
    "        print(\"   - No checkpoint found (need to train first or download pretrained)\")\n",
    "    print(\"\\nüí° You can still test provenance graph and seeding (see cells above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_6",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Pipeline (Requires Engine + Datasets)\n",
    "\n",
    "### 6.1 Check Training Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_training_prereqs",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_exists = (Path(\"src/engine/graph_matcher/engine_repo/src\").exists())\n",
    "datasets_exist = (Path(\"data/datasets/darpa_cadets\").exists())\n",
    "\n",
    "print(\"üîç Training Prerequisites:\")\n",
    "print(f\"   Engine: {'‚úÖ' if engine_exists else '‚ùå'}\")\n",
    "print(f\"   DARPA Datasets: {'‚úÖ' if datasets_exist else '‚ùå'}\")\n",
    "\n",
    "if engine_exists and datasets_exist:\n",
    "    print(\"\\n‚úÖ Ready to train!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Missing prerequisites for training\")\n",
    "    if not engine_exists:\n",
    "        print(\"   - Install engine (see section 3.1)\")\n",
    "    if not datasets_exist:\n",
    "        print(\"   - Download DARPA TC datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_6_2",
   "metadata": {},
   "source": [
    "### 6.2 Run Training (if prerequisites met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "if engine_exists and datasets_exist:\n",
    "    print(\"üéì Starting training...\")\n",
    "    \n",
    "    Path(\"runs/checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"src.pipeline.train.trainer\",\n",
    "        \"--dataset\", \"cadets\",\n",
    "        \"--epochs\", \"10\",  # Small number for demo\n",
    "        \"--save\", \"runs/checkpoints/demo_model.pt\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Command: {' '.join(cmd)}\\n\")\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n‚úÖ Training completed!\")\n",
    "    else:\n",
    "        print(\"\\nErrors:\")\n",
    "        print(result.stderr)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping training (prerequisites not met)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_7",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluation & Metrics\n",
    "\n",
    "### 7.1 Evaluate CTI Agent (Custom Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "seeds_file = Path(\"runs/cti/seeds.json\")\n",
    "\n",
    "if seeds_file.exists():\n",
    "    seeds = json.loads(seeds_file.read_text())\n",
    "    techniques = seeds.get('techniques', [])\n",
    "    indicators = seeds.get('indicators', [])\n",
    "    \n",
    "    print(\"üìä CTI Agent Evaluation Metrics:\\n\")\n",
    "    \n",
    "    # 1. Quantity metrics\n",
    "    print(f\"1. Extraction Counts:\")\n",
    "    print(f\"   - Techniques: {len(techniques)}\")\n",
    "    print(f\"   - Indicators: {len(indicators)}\")\n",
    "    \n",
    "    # 2. Technique distribution\n",
    "    print(f\"\\n2. Technique Distribution:\")\n",
    "    tech_ids = [t.get('technique_id') for t in techniques]\n",
    "    tech_counts = Counter(tech_ids)\n",
    "    for tid, count in tech_counts.most_common(10):\n",
    "        print(f\"   {tid}: {count} occurrences\")\n",
    "    \n",
    "    # 3. Confidence distribution\n",
    "    print(f\"\\n3. Confidence Distribution:\")\n",
    "    confidences = [t.get('confidence', 0) for t in techniques]\n",
    "    if confidences:\n",
    "        import numpy as np\n",
    "        print(f\"   Mean: {np.mean(confidences):.3f}\")\n",
    "        print(f\"   Median: {np.median(confidences):.3f}\")\n",
    "        print(f\"   Std: {np.std(confidences):.3f}\")\n",
    "        print(f\"   Min: {np.min(confidences):.3f}\")\n",
    "        print(f\"   Max: {np.max(confidences):.3f}\")\n",
    "    \n",
    "    # 4. Indicator types\n",
    "    print(f\"\\n4. Indicator Types:\")\n",
    "    ind_types = Counter([i.get('type') for i in indicators])\n",
    "    for itype, count in ind_types.items():\n",
    "        print(f\"   {itype}: {count}\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    if confidences:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Confidence histogram\n",
    "        axes[0].hist(confidences, bins=20, edgecolor='black')\n",
    "        axes[0].set_xlabel('Confidence')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title('Technique Confidence Distribution')\n",
    "        axes[0].axvline(np.mean(confidences), color='r', \n",
    "                       linestyle='--', label=f'Mean: {np.mean(confidences):.2f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Top techniques\n",
    "        top_techs = tech_counts.most_common(10)\n",
    "        if top_techs:\n",
    "            tids, counts = zip(*top_techs)\n",
    "            axes[1].barh(range(len(tids)), counts)\n",
    "            axes[1].set_yticks(range(len(tids)))\n",
    "            axes[1].set_yticklabels(tids)\n",
    "            axes[1].set_xlabel('Occurrences')\n",
    "            axes[1].set_title('Top 10 Techniques')\n",
    "            axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No seeds.json found - run CTI Agent first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_8",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ System Status Dashboard\n",
    "\n",
    "### Complete System Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "system_dashboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def check_file(path, desc):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        size = p.stat().st_size if p.is_file() else \"N/A\"\n",
    "        return f\"‚úÖ {desc}: {p} ({size} bytes)\"\n",
    "    return f\"‚ùå {desc}: {path} NOT FOUND\"\n",
    "\n",
    "def check_dir(path, desc):\n",
    "    p = Path(path)\n",
    "    if p.exists() and p.is_dir():\n",
    "        count = len(list(p.iterdir()))\n",
    "        return f\"‚úÖ {desc}: {p} ({count} items)\"\n",
    "    return f\"‚ùå {desc}: {path} NOT FOUND\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" üõ°Ô∏è  APT ATTACK DETECTION - SYSTEM STATUS DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüì¶ Data Files:\")\n",
    "print(check_file(\"data/mitre/enterprise-attack.json\", \"MITRE ATT&CK\"))\n",
    "print(check_file(\"data/cti_reports/rss_seeds.txt\", \"RSS Feeds\"))\n",
    "print(check_file(\"runs/events/events.jsonl\", \"Sample Events\"))\n",
    "\n",
    "print(\"\\nü§ñ Engine & Datasets:\")\n",
    "print(check_dir(\"src/engine/graph_matcher/engine_repo\", \"GNN Engine\"))\n",
    "print(check_dir(\"data/datasets\", \"DARPA Datasets\"))\n",
    "\n",
    "print(\"\\nüìä Outputs:\")\n",
    "print(check_file(\"runs/cti/seeds.json\", \"CTI Seeds\"))\n",
    "print(check_dir(\"data/query_graphs\", \"Query Graphs\"))\n",
    "print(check_dir(\"runs/checkpoints\", \"Model Checkpoints\"))\n",
    "\n",
    "print(\"\\nüîß Python Packages:\")\n",
    "packages = ['networkx', 'yaml', 'feedparser', 'openai', 'g4f', 'torch']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"   ‚úÖ {pkg}\")\n",
    "    except:\n",
    "        print(f\"   ‚ùå {pkg}\")\n",
    "\n",
    "print(\"\\nüö¶ Pipeline Status:\")\n",
    "status = []\n",
    "status.append((\"CTI Agent\", \n",
    "               Path(\"data/mitre/enterprise-attack.json\").exists() and \n",
    "               Path(\"data/cti_reports/rss_seeds.txt\").exists()))\n",
    "status.append((\"Hunting\", \n",
    "               Path(\"src/engine/graph_matcher/engine_repo/src\").exists() and\n",
    "               Path(\"runs/events/events.jsonl\").exists()))\n",
    "status.append((\"Training\", \n",
    "               Path(\"src/engine/graph_matcher/engine_repo/src\").exists() and\n",
    "               Path(\"data/datasets\").exists()))\n",
    "\n",
    "for name, ready in status:\n",
    "    symbol = \"‚úÖ READY\" if ready else \"‚ö†Ô∏è  NOT READY\"\n",
    "    print(f\"   {symbol}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Summary\n",
    "ready_count = sum(1 for _, ready in status if ready)\n",
    "if ready_count == 3:\n",
    "    print(\"üéâ ALL PIPELINES READY!\")\n",
    "elif ready_count >= 1:\n",
    "    print(f\"‚úÖ {ready_count}/3 pipelines ready\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Setup incomplete - see missing items above\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_9",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Troubleshooting & Help\n",
    "\n",
    "### Common Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "troubleshooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üîß TROUBLESHOOTING GUIDE\n",
    "========================\n",
    "\n",
    "1. \"OPENAI_API_KEY not set\"\n",
    "   Solution: Use g4f backend or set API key:\n",
    "   >>> os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "\n",
    "2. \"Engine not found\"\n",
    "   Solution: Bootstrap engine:\n",
    "   $ bash scripts/bootstrap_engine.sh <MEGR_APT_GIT_URL>\n",
    "\n",
    "3. \"No module named 'torch'\"\n",
    "   Solution: Install hunting requirements:\n",
    "   $ pip install -r requirements/hunting.txt\n",
    "\n",
    "4. \"g4f not available\"\n",
    "   Solution: Install g4f:\n",
    "   $ pip install -r requirements/g4f.txt\n",
    "\n",
    "5. \"No CTI results\"\n",
    "   Possible causes:\n",
    "   - RSS feeds unreachable (check network)\n",
    "   - LLM rate limiting (wait and retry)\n",
    "   - Invalid STIX data (re-download MITRE file)\n",
    "\n",
    "6. \"Graph is empty\"\n",
    "   Solution: Create sample events first (see section 2.3)\n",
    "\n",
    "7. \"Cannot run hunting\"\n",
    "   Requirements:\n",
    "   - Engine installed\n",
    "   - Checkpoint file (.pt)\n",
    "   - Events data\n",
    "\n",
    "üìö For more help, see:\n",
    "   - README.md\n",
    "   - ANALYSIS_AND_GAPS.md\n",
    "   - GitHub issues\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Next Steps\n",
    "\n",
    "1. **For CTI Agent Testing**:\n",
    "   - Run sections 1-4\n",
    "   - Experiment with different RSS feeds\n",
    "   - Compare OpenAI vs g4f results\n",
    "\n",
    "2. **For Full System**:\n",
    "   - Bootstrap engine (section 3)\n",
    "   - Obtain DARPA datasets\n",
    "   - Train models (section 6)\n",
    "   - Run hunting pipeline (section 5)\n",
    "\n",
    "3. **For Evaluation**:\n",
    "   - Implement `src/eval/agent_eval.py`\n",
    "   - Implement `src/eval/hunting_eval.py`\n",
    "   - Create ground truth datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Version**: 1.0  \n",
    "**Last Updated**: 2026-01-04  \n",
    "**Author**: APT Detection Team\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
