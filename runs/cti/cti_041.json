{
  "title": "How CrowdStrike Trains GenAI Models at Scale Using Distributed Computing",
  "link": "https://www.crowdstrike.com/en-us/blog/how-crowdstrike-trains-genai-models-at-scale-using-distributed-computing/",
  "published": "Dec 22, 2025 00:00:00-0600",
  "text": "How CrowdStrike Trains GenAI Models with Distributed Computing BLOG Featured Streamline Security Operations with Falcon for IT’s Turnkey Automations Dec 29, 2025 How CrowdStrike Trains GenAI Models at Scale Using Distributed Computing Dec 22, 2025 Inside CrowdStrike’s Science-Backed Approach to Building Expert SOC Agents Dec 19, 2025 CrowdTour 2026: Securing the AI Era Together Dec 19, 2025 Recent Streamline Security Operations with Falcon for IT’s Turnkey Automations Dec 29, 2025 How CrowdStrike Trains GenAI Models at Scale Using Distributed Computing Dec 22, 2025 Inside CrowdStrike’s Science-Backed Approach to Building Expert SOC Agents Dec 19, 2025 CrowdTour 2026: Securing the AI Era Together Dec 19, 2025 Video Video Highlights the 4 Key Steps to Successful Incident Response Dec 02, 2019 Helping Non-Security Stakeholders Understand ATT&CK in 10 Minutes or Less [VIDEO] Feb 21, 2019 Analyzing Targeted Intrusions Through the ATT&CK Framework Lens [VIDEO] Jan 22, 2019 Qatar’s Commercial Bank Chooses CrowdStrike Falcon®: A Partnership Based on Trust [VIDEO] Aug 20, 2018 Category Agentic SOC Agentic SOC How CrowdStrike Trains GenAI Models at Scale Using Distributed Computing 12/22/25 Inside CrowdStrike’s Science-Backed Approach to Building Expert SOC Agents 12/19/25 CrowdTour 2026: Securing the AI Era Together 12/19/25 CrowdStrike Leverages NVIDIA Nemotron in Amazon Bedrock to Advance Agentic Security 12/02/25 Cloud & Application Security Cloud & Application Security CrowdStrike Unveils Real-Time Cloud Detection and Response Innovations 12/01/25 New User Experience Transforms Interaction with the Falcon Platform 10/21/25 How Falcon ASPM Secures GenAI Applications and Lessons from Dogfooding 09/30/25 CrowdStrike Named a Frost Radar™ Leader in Cloud Workload Protection Platforms 09/25/25 Threat Hunting & Intel Threat Hunting & Intel Unveiling WARP PANDA: A New Sophisticated China-Nexus Adversary 12/04/25 CrowdStrike Research: Security Flaws in DeepSeek-Generated Code Linked to Political Triggers 11/20/25 CrowdStrike 2025 European Threat Landscape Report: Extortion Rises, Nation-State Activity Intensifies 11/03/25 CrowdStrike 2025 APJ eCrime Landscape Report: A New Era of Threats Emerges 10/20/25 Endpoint Security & XDR Endpoint Security & XDR Streamline Security Operations with Falcon for IT’s Turnkey Automations 12/29/25 CrowdStrike Endpoint Security Achieves 273% ROI Over Three Years 12/17/25 CrowdStrike Achieves 100% Detection, 100% Protection, and Zero False Positives in 2025 MITRE ATT&CK® Enterprise Evaluations 12/10/25 Defeating BLOCKADE SPIDER: How CrowdStrike Stops Cross-Domain Attacks 11/18/25 Engineering & Tech Engineering & Tech EMBER2024: Advancing the Training of Cybersecurity ML Models Against Evasive Malware 09/03/25 Falcon Platform Prevents COOKIE SPIDER’s SHAMOS Delivery on macOS 08/20/25 CrowdStrike’s Approach to Better Machine Learning Evaluation Using Strategic Data Splitting 08/11/25 CrowdStrike Researchers Develop Custom XGBoost Objective to Improve ML Model Release Stability 03/20/25 Executive Viewpoint Executive Viewpoint The Dawn of the Agentic SOC: Reimagining Cybersecurity for the AI Era 09/26/25 CrowdStrike Falcon Platform Evolves to Lead the Agentic Security Era 09/16/25 CrowdStrike to Acquire Pangea to Secure Enterprise AI Use and Development 09/15/25 CrowdStrike to Acquire Onum to Transform How Data Powers the Agentic SOC 08/27/25 From The Front Lines From The Front Lines CrowdStrike Named a Leader in 2025 IDC MarketScape for Worldwide Incident Response Services 08/27/25 CrowdStrike Launches New AI Security Services to Strengthen AI Security and SOC Readiness 08/06/25 CrowdStrike Detects and Blocks Initial SharePoint Zero-Day Exploitation 07/21/25 Prescription for Protection: Healthcare Industry Observations from CrowdStrike Investigations 07/01/25 Next-Gen Identity Security Next-Gen Identity Security Falcon Shield Evolves with AI Agent Visibility and Falcon Next-Gen SIEM Integration 12/08/25 CrowdStrike Named Overall Leader in 2025 KuppingerCole ITDR Leadership Compass 11/10/25 CrowdStrike Named the Leader in 2025 Frost Radar for SaaS Security Posture Management 10/27/25 From Domain User to SYSTEM: Analyzing the NTLM LDAP Authentication Bypass Vulnerability (CVE-2025-54918) 10/22/25 Next-Gen SIEM & Log Management Next-Gen SIEM & Log Management Transform AWS Security Operations with Falcon Next-Gen SIEM 12/01/25 CrowdStrike Leads New Evolution of Security Automation with Charlotte Agentic SOAR 11/05/25 CrowdStrike Named a Visionary in 2025 Gartner® Magic Quadrant™ for Security Information and Event Management 10/10/25 CrowdStrike Boosts SOC Detection Content with Correlation Rule Template Discovery Dashboard 09/29/25 Public Sector Public Sector CrowdStrike Achieves FedRAMP® High Authorization 03/19/25 NHS Matures Healthcare Cybersecurity with NCSC’s CAF Assurance Model 03/13/25 Zero Trust Strengthens Data Protection to Achieve National Cyber Strategy Goals 01/15/25 CrowdStrike Statement on Bloomberg’s October 25, 2024 Story 10/25/24 Exposure Management Exposure Management Inside the Latest Innovations Powering Falcon Exposure Management 12/12/25 December 2025 Patch Tuesday: One Critical Zero-Day, Two Publicly Disclosed Vulnerabilities Among 57 CVEs 12/09/25 From Vulnerability Management to Exposure Management: The Platform Era Has Arrived 11/13/25 November 2025 Patch Tuesday: One Zero-Day and Five Critical Vulnerabilities Among 63 CVEs 11/12/25 Data Protection Data Protection Falcon Data Protection for Cloud Extends DSPM into Runtime 11/20/25 CrowdStrike Stops GenAI Data Leaks with Unified Data Protection 09/18/25 Q&A: How Mastronardi Produce Secures Innovation with CrowdStrike 02/14/25 5 Key Data Protection Challenges and How to Overcome Them 12/11/24 Securing AI Securing AI CrowdStrike Secures Growing AI Attack Surface with Falcon AI Detection and Response 12/15/25 Data Leakage: AI’s Plumbing Problem 12/11/25 Indirect Prompt Injection Attacks: A Lurking Risk to AI Systems 12/04/25 Secure AI at Machine Speed: Defending the Growing Attack Surface 09/02/25 Start Free Trial Featured Recent Video Category Start Free Trial How CrowdStrike Trains GenAI Models at Scale Using Distributed Computing CrowdStrike researchers outline key concepts, tools, and techniques used in LLM training December 22, 2025 | Andrei Preda - Alexandru Dinu - Florian Stortz - Nathan Nusaputra - Catalin-Andrei Stan | Agentic SOC • Engineering & Tech Large language models (LLMs) have revolutionized artificial intelligence and are rapidly transforming the cybersecurity landscape. As these powerful models become commonly used among both attackers and defenders, developing specialized cybersecurity LLMs has become a strategic imperative. The CrowdStrike 2025 Global Threat Report highlights a concerning trend: Threat actors are increasingly enhancing social engineering and computer network operations campaigns with LLM capabilities. To counter these evolving threats, CrowdStrike is investing in custom LLMs purpose-built for cybersecurity applications, designed to understand and address the unique challenges of this domain. Here, we discuss key concepts in LLM training and offer insights into CrowdStrike's approach to developing next-generation security-focused language models. A team of our data scientists gave a presentation on this topic as part of a Customer Voice segment at the Google Cloud Next 2025 conference, where CrowdStrike received the 2025 Google Cloud Security Partner of the Year Award for Workload Security . Infrastructure for LLM Experimentation at Scale Today's state-of-the-art LLMs, which excel at tasks from general knowledge to coding and reasoning, require immense computational resources. Training these models demands high-performance, distributed computing clusters at a scale. To meet demand, major AI labs are announcing increasingly larger infrastructures, with some even exploring nuclear energy solutions to fuel their energy needs. Setting up such a training cluster is the essential first step in developing LLMs at scale. CrowdStrike leverages Google Cloud Vertex Training Platform, specifically Vertex Training Cluster to streamline training cluster management. We start with a small number of multi-GPU instances (nodes) for testing and efficiently scale up to larger node counts as needed. Our infrastructure-as-code approach enables seamless operations, simplified maintenance, and continuous security updates. Our custom dashboards and automated alerts, built on Google Cloud’s real-time metrics, help to ensure consistent cluster performance and reliability. Lastly, we are able to partition and spin up dedicated large clusters for higher priority jobs enabling multiple teams to efficiently run their large training workloads. With the cluster infrastructure in place through the Vertex Training Cluster, we configured Slurm, the industry-standard workload manager for high-performance computing, to handle job scheduling and resource allocation. This allows our researchers to focus on innovation rather than infrastructure management, enabling concurrent work across multiple projects. We maintain dedicated Slurm partitions for both interactive and non-interactive workloads. This separation ensures there's always a pool of resources available for daily data exploration and small-scale processing tasks, while preventing longer training jobs from disrupting interactive work. Practical Considerations for Training LLMs LLMs are trained with the causal language modelling objective, i.e., “predicting the next token in a sequence, given previous tokens.” While conceptually straightforward, implementation requires addressing several practical challenges. In this section, we discuss how CrowdStrike has tackled some of these challenges so far. Data Strategy As with any machine learning application, high-quality, diverse datasets are fundamental to LLM performance. At CrowdStrike, we enhance training data through synthetic generation by guiding LLMs to generate, augment, reinterpret, or reword certain inputs, such as documents and code. Our researchers have produced more robust models after including synthetic data, especially for low-resource concepts, like domain-specific languages. Our optimized generation process implements a complex pipeline that leverages a feedback loop with validation, filtering, and regeneration steps. Parallelizing the workload and taking advantage of the hardware is critical for timely results and a tight feedback loop. CrowdStrike’s modular data pipelines assemble large-scale datasets by using recipes (specifications) and pulling data from multiple sources. For experimental cases where the dataset is not fixed and may grow over time, the data pipeline can effectively stream and construct training batches just-in-time for model training. Distributed Computing for LLM training LLMs are trained on large-scale clusters using distributed computing. Therefore, multiple algorithms for parallelism have been devised, including: Data parallelism (replicating models across nodes for parallel processing of multiple data subsets) Tensor parallelism (sharding tensors ) Pipeline parallelism (sharding different model layers across nodes) These approaches reduce memory requirements and enable training larger models with less hardware. Context parallelism and expert parallelism further help manage longer contexts and train mixture-of-experts architectures. Together, these five approaches constitute “5D parallelism,” with practitioners typically using a subset of them based on specific needs. CrowdStrike researchers apply these techniques selectively, depending on project specifics like model size and available hardware. As the HuggingFace Ultrascale Playbook notes, “we can’t give a single unified recipe” since optimal approaches depend on specific hardware configurations. Similarly, there are many frameworks that implement training algorithms, such as PyTorch (FSDP), Microsoft’s DeepSpeed, and HuggingFace’s Accelerate, which allows users to interface easily with both. Hardware-specific Optimizations Given Training Configurations To illustrate the importance of tailoring solutions to specific hardware, we compared identical training parameters across different GPU architectures. We tested two attention mechanism implementations on NVIDIA's H100 and B200 GPUs, using single GCE A3 Mega and A4 nodes, respectively. While Flash Attention 2 consistently outperformed scaled dot-product attention (SDPA), the magnitude varied significantly between architectures. One configuration showed only modest acceleration, while the other went from slowest to fastest performer when the attention mechanism was optimized. These results, though specific to our experimental conditions (e.g., LLM architecture, attention mechanism, or context size requirements), highlight a key takeaway: Meaningful performance gains require optimizations that consider the complete software and hardware ecosystem. Figure 1. Shows the training speed for two GPU architectures in combination with two attention implementations. Flash Attention 2 brings a small improvement to the older GPU architecture, but it makes a significant difference for the newer one. With Flash Attention 2, the newer architecture goes from being the slowest option to being the fastest one. Figure 2. Similar to Figure 1 but for a longer context length (64k tokens). The longer context length causes speeds to be slower in absolute terms, but all observations made in Figure 1 still hold. Node Communication For effective large-scale parallelism during training, cluster nodes need to communicate and synchronize with each other (e.g., when updating model weights in the backward pass). CrowdStrike leverages optimized operations from NVIDIA’s NCCL for fast inter-node communication. Through careful configuration, we’ve achieved accelerations of up to 6x over baseline performance. Extended Context Handling At CrowdStrike, we experiment with LLMs for binary analysis (for example, Byte Back: Next-Generation Malware Classification Using Binary Transformers ). This work requires significantly longer contexts than typical text applications to accommodate the tokenized representation of the byte data from the files. Since the classic Transformer softmax attention has a quadratic complexity in both time and space with respect to the sequence length, we employ context parallelism across multiple GPUs. Context parallelism comes in multiple forms, such as ring attention , DeepSpeed Ulysses, and Ulysses-Offload , and in multiple frameworks including NVIDIA NeMo and DeepSpeed-Megatron . We have experienced good results with the DeepSpeed Ulysses sequence parallelism . This methodology splits dataset entries along the sequence dimension among multiple GPUs, and all-to-all communication is used to compute the attention in parallel on the head dimension. Combining sequence parallelism with a high performance attention implementation (such as Flash Attention ), and with a memory optimized loss computation (such as fused cross-entropy loss ) or by computing the loss in blocks of logits (proposed by ProLong ), leads to an effective training throughput in our distributed environment. Figure 3. Shows training throughput for Llama 3.1 8B using various long-context scenarios. Doubling the context size roughly halves the training throughput. Training LLMs with long contexts is an ongoing challenge with respect to performance, particularly due to the inherent complexity of the task and the constant necessity for software to keep pace with hardware advancements. In the example of Llama 3.1B above, doubling context size roughly halved training throughput. However, this ratio doesn’t necessarily hold true for larger architecture LLMs. We are continually conducting experiments to optimize performance, also accounting for the GPU configurations. Memory Management and Gradient Checkpointing Hardware constraints, particularly video RAM (VRAM) limitations, dictate feasible model architectures and configurations as well as training times. To fit larger models or batches into limited memory, we employ gradient checkpointing. During training, deep learning models undergo forward and backward passes, computing gradients needed to update the model's parameters and applying these updates, respectively. Since gradients are typically computed for each parameter, a model effectively doubles its VRAM footprint during training. Gradient checkpointing \"forgets\" certain gradients to reduce peak memory usage, recomputing them during the backward pass when needed. This technique trades additional computation for reduced memory requirements, a worthwhile compromise when VRAM is constrained. Figure 4. Shows how activating gradient checkpointing affects peak VRAM requirements for various distributed algorithms (FSDP and variants of DeepSpeed ZeRO). The reduction in peak VRAM is different for each algorithm, with DeepSpeed ZeRO 3 seeing the largest reduction (80%, from 31GB down to 6GB). Figure 5. Shows how activating gradient checkpointing affects training time for the distributed algorithms in Figure 4. In all cases, gradient checkpointing slows down training. However, this slowdown is not prohibitive for this specific test workload. Observability Monitoring is essential for both small- and large-scale training runs to optimize configurations and troubleshoot issues. CrowdStrike utilizes PyTorch's profiler to analyze resource usage and maximize hardware efficiency, tracking metrics like SM Efficiency for NVIDIA GPUs, which estimates the fraction of active processors. We complement this with Google Cloud’s dashboards to monitor node status and detect potential hardware failures, an inevitable occurrence during extended training of large models. Managing the Complexity of Training LLMs at Scale Training LLMs at scale involves numerous complex, interconnected components. Based on our experience, we recommend: Prioritize caching and reusability for both code and data to reduce duplication. Maintain relevant metadata for datasets for reproducibility and auditing. Validate at small scale with tight feedback loops before scaling up. Use log extensive metrics to improve visibility, debugging, and model selection. Configure resource usage-based alerts to proactively prevent failures. Invest in robust data pipelines and management to handle multiple sources, custom pre-processing, data mixing, and synthetic data generation. Enforce best practices through testing and automation to build trust and efficiency. LLM Research at CrowdStrike At CrowdStrike, generative AI is one of our primary areas of research. Teams of data scientists study the development and usage of various types of language models and training techniques. The setup described in this article needs to support fine-tuning agentic models, training foundational models, fast distributed inference, and many other use cases. As our projects become more ambitious and our execution capabilities scale, we look forward to unlocking innovative ways of stopping the adversary. One example, building on the insights gained from prior research into Binary Transformers for malware classification, is the large byte model (LBM). CrowdStrike is taking the next step by developing a multi-modal LLM specifically designed for in-depth binary file analysis. Additional Resources Watch this CrowdStrike presentation on using Google Cloud for large scale LLM training . Learn more about CrowdStrike’s innovations in AI and machine learning . Read how AI is powering the Falcon agentic security platform in this blog: CrowdStrike’s Fall 2025 Release Defines the Agentic SOC and Secures the AI Era . Tweet Share Related Content Inside CrowdStrike’s Science-Backed Approach to Building Expert SOC Agents CrowdTour 2026: Securing the AI Era Together CrowdStrike Leverages NVIDIA Nemotron in Amazon Bedrock to Advance Agentic Security Categories Agentic SOC 44 Cloud & Application Security 137 Data Protection 18 Endpoint Security & XDR 345 Engineering & Tech 85 Executive Viewpoint 176 Exposure Management 112 From The Front Lines 197 Next-Gen Identity Security 62 Next-Gen SIEM & Log Management 108 Public Sector 40 Securing AI 19 Threat Hunting & Intel 206 CONNECT WITH US FEATURED ARTICLES October 01, 2024 CrowdStrike Named a Leader in 2024 Gartner® Magic Quadrant™ for Endpoint Protection Platforms September 25, 2024 Recognizing the Resilience of the CrowdStrike Community September 25, 2024 CrowdStrike Drives Cybersecurity Forward with New Innovations Spanning AI, Cloud, Next-Gen SIEM and Identity Protection September 18, 2024 SUBSCRIBE Sign up now to receive the latest notifications and updates from CrowdStrike. Sign Up See CrowdStrike Falcon ® in Action Detect, prevent, and respond to attacks— even malware-free intrusions—at any stage, with next-generation endpoint protection. See Demo Inside CrowdStrike’s Science-Backed Approach to Building Expert SOC Agents Copyright © 2025 CrowdStrike Privacy Request Info Blog Contact Us 1.888.512.8906 Accessibility"
}